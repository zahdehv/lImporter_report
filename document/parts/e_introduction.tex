\chapter{Introducción}
\label{chapter:introduccion}

% La gestión del conocimiento se ha convertido en una piedra angular en la era de la información, donde la capacidad de organizar, sintetizar y recuperar datos de manera eficiente es crucial para el desarrollo personal y profesional.
La sobrecarga de información en la actualidad plantea un desafío significativo para la productividad y el aprendizaje efectivo. En este contexto, la gestión del conocimiento personal emerge como una disciplina fundamental, no solo para académicos e investigadores, sino para cualquier individuo que busque optimizar su capacidad de aprendizaje y generación de ideas. Como señala \cite{ahrensHowTakeSmart2017}, una gestión eficaz de las notas y el conocimiento adquirido no solo facilita la escritura y el estudio, sino que transforma la manera en que se interactúa con la información, convirtiéndola en un activo dinámico y generador de nuevas perspectivas. El desarrollo de un \textit{segundo cerebro} \parencite{forteBuildingSecondBrain2022}, un sistema externo confiable para almacenar y conectar ideas, libera recursos cognitivos, permitiendo un enfoque más profundo en el pensamiento crítico y la creatividad.
El potencial de los Grandes Modelos de Lenguaje (\textit{LLM}) para revolucionar este campo es inmenso. Estas tecnologías ofrecen la posibilidad de automatizar y enriquecer la integración del conocimiento, asistiendo en la destilación de información, la identificación de conexiones y la generación de contenido relevante dentro de las bases de conocimiento personales. La automatización de estos procesos no solo promete un aumento en la eficiencia, sino también una democratización del acceso a metodologías avanzadas de gestión del conocimiento.

\section{Gestión del Conocimiento Personal}
\label{sec:pkm}
La Gestión del Conocimiento Personal (\textit{PKM}, por sus siglas en inglés) se define como el proceso mediante el cual un individuo recopila, clasifica, almacena, busca, recupera y comparte conocimiento en sus actividades diarias \parencite{grundspenkisAgentBasedApproach2007}. Esta disciplina es particularmente relevante para los denominados \textit{knowledge workers}, profesionales para quienes el conocimiento es su activo más valioso y que dedican una parte significativa de su tiempo a gestionar grandes cantidades de información. La PKM busca empoderar a estos individuos para que sean más efectivos en sus entornos personales y sociales.

\subsection{Toma de notas}
La toma de notas, lejos de ser un mero acto de transcripción, es una estrategia fundamental para mejorar el aprendizaje y la retención de información \parencite{jansenIntegrativeReviewCognitive2017}. Al tomar notas, el individuo no solo registra la esencia de la información, sino que también se involucra en un proceso activo de filtrado, organización y reestructuración del conocimiento.

Existen diversas metodologías para la toma de notas, que pueden clasificarse ampliamente en lineales y no lineales.
Las metodologías lineales implican registrar la información en el orden en que se recibe. Un ejemplo común es denominado \textit{outlining}, donde las notas y pensamientos se organizan de manera estructurada y lógica. Otra técnica lineal es el \textit{sentence method}, que consiste en anotar cada tema como una oración corta y simple, ideal para lecciones de ritmo rápido.

Las metodologías no lineales, por otro lado, utilizan la organización espacial y los diagramas para ensamblar la información. Entre estas se encuentra el \textit{charting}, útil para temas que pueden desglosarse en categorías. El \textit{mapping}, como los mapas mentales, organiza las ideas en una estructura de árbol a partir de un punto central. Las Notas Cornell, desarrolladas por Walter Pauk \parencite{paukHowStudyCollege2010}, dividen la página en secciones para notas, pistas y un resumen. Las \textit{guided notes} proporcionan un esquema predefinido con puntos clave faltantes que el estudiante completa.
Más allá de estas técnicas tradicionales, han surgido sistemas más sofisticados. El método \textit{Zettelkasten}, popularizado por Niklas Luhmann y descrito en detalle por \cite{ahrensHowTakeSmart2017}, se basa en la creación de notas atómicas interconectadas, formando una red de conocimiento que fomenta la generación de ideas y la escritura prolífica. Este sistema, aunque con raíces históricas, ha ganado nueva relevancia en la era digital. Por otro lado, el método PARA \textit{(Projects, Areas, Resources, Archives)}, propuesto por \cite{forteBuildingSecondBrain2022}, ofrece un marco para organizar la información digital en función de su accionabilidad, facilitando la gestión de proyectos y responsabilidades a largo plazo.

\subsection{Bases de Conocimiento Personal}
\label{subsec:pkb}
Una Base de Conocimiento Personal (\textit{PKB}) es una herramienta electrónica utilizada por un individuo para expresar, capturar y recuperar conocimiento personal. Se diferencia de una base de datos tradicional al contener material subjetivo y específico del propietario. El concepto de extender la memoria y las capacidades cognitivas del individuo mediante herramientas externas no es nuevo. Ya en 1945, Vannevar Bush imaginó el \textit{Memex} (de \textit{memory extension}), un dispositivo electromecánico en el que un individuo almacenaría todos sus libros, registros y comunicaciones, mecanizado de tal manera que pudiera ser consultado con gran velocidad y flexibilidad \parencite{bushWeMayThink1945}. Bush previó un futuro donde la sobrecarga de información requeriría nuevas formas de acceder y conectar el conocimiento acumulado.

La forma en que una PKB organiza el conocimiento se define por su modelo de datos. Estudios como los de Davies y colegas \parencite{daviesBuildingMemexSixty2005, daviesStillBuildingMemex2011} han analizado estos modelos en función de su marco estructural (cómo se interrelacionan los elementos), los elementos de conocimiento (las unidades básicas de información) y su esquema (el nivel de semántica formal). Un aspecto crucial destacado es la transclusión, la capacidad de ver el mismo elemento de conocimiento en múltiples contextos sin duplicación.

\paragraph{Grafos de Conocimiento Personal (\textit{PKG})}
Dentro de las PKB, los Grafos de Conocimiento Personal han ganado prominencia. Un PKG representa el conocimiento como una red de nodos interconectados, donde cada nodo es una pieza de información y las aristas representan las relaciones entre ellas, a menudo con visualizaciones gráficas que facilitan la exploración y el descubrimiento de conexiones \parencite{pyneMetaworkHowWe2022}.

\subsection{Sistemas Digitales de Toma de Notas}
\label{subsec:sistemas_digitales_toma_notas}
La visión del \textit{Memex} de Bush encuentra un eco contemporáneo en la plétora de sistemas digitales de toma de notas. Estos sistemas varían ampliamente en sus características y enfoques. Algunos, como \textit{Roam Research} y \textit{Logseq}, enfatizan los enlaces bidireccionales y la visualización gráfica, alineándose estrechamente con la idea de un \textit{PKG}. Otros, como \textit{Notion}, ofrecen una gran flexibilidad para crear bases de datos y vistas personalizadas, mientras que \textit{Evernote} y \textit{Google Keep} son populares por su simplicidad y accesibilidad multiplataforma. \textit{Microsoft OneNote} tiene casi todos los \textit{checks} en el articulo de la \textit{Wikipedia}. Alternativas de código abierto como \textit{Zettlr} y \textit{Joplin} también ofrecen robustas funcionalidades, a menudo con un enfoque en la privacidad y el control local de los datos. Herramientas más tradicionales como \textit{VimWiki} (para usuarios de \textit{Vim}) y \textit{OrgMode} (para \textit{Emacs}) ofrecen sistemas de toma de notas altamente personalizables y potentes para usuarios con conocimientos técnicos. \textit{Markor} y \textit{SimpleNote} se centran en la simplicidad y la edición \textit{Markdown}.

Entre estas herramientas, \textit{Obsidian.md} ha ganado una considerable popularidad para la escritura académica y la gestión del conocimiento personal. Siguiendo el modelo de datos de las \textit{PKB}, \textit{Obsidian} almacena las notas como archivos locales de texto plano en formato \textit{Markdown}, lo que garantiza la portabilidad y la longevidad de los datos. Su marco estructural se basa en un grafo de conocimiento, donde cada nota es un nodo y los enlaces bidireccionales permiten crear una red interconectada de información. Esto facilita la \textit{transclusión}, ya que una misma idea o nota puede ser referenciada y contextualizada desde múltiples puntos del grafo sin necesidad de duplicación. Los elementos de conocimiento son flexibles, desde conceptos simples hasta notas extensas. Obsidian también soporta un esquema enriquecido mediante el uso de metadatos (\textit{frontmatter}), etiquetas y la posibilidad de extender su funcionalidad mediante \textit{plugins}, como \textit{Dataview}, que permite realizar consultas complejas sobre las notas. Su vista de grafo visualiza las conexiones, ayudando a identificar relaciones y patrones emergentes.

\section{Grandes Modelos de Lenguaje}
\label{sec:llm}
Los Grandes Modelos de Lenguaje (\textit{LLM}) representan un avance transformador en el campo de la inteligencia artificial, con la capacidad de comprender, generar y manipular el lenguaje natural a niveles sin precedentes. Modelos como \textit{GPT-4} de \textit{OpenAI} \parencite{openaiGPT4TechnicalReport2024} y \textit{Gemini} de \textit{Google} \parencite{teamGeminiFamilyHighly2024} han demostrado habilidades notables en una amplia gama de tareas lingüísticas. En paralelo, la comunidad de código abierto ha respondido con modelos competitivos como \textit{DeepSeek} \parencite{deepseek-aiDeepSeekV3TechnicalReport2024}, \textit{Qwen} \parencite{baiQwenTechnicalReport2023} y \textit{LLaMa} \parencite{grattafioriLlama3Herd2024}, impulsando la innovación y la accesibilidad en este campo.

\subsection{Técnicas de \textit{Prompting}}
\label{subsec:prompting_techniques}
Para interactuar eficazmente con los LLM, se han desarrollado un sinfín de técnicas de \textit{prompting}. Inicialmente, se descubrió que proporcionar unos pocos ejemplos (\textit{few-shot learning}) mejoraba significativamente los resultados, apelando a la capacidad de generalización de los modelos sin necesidad de reentrenamiento \parencite{brownLanguageModelsAre2020}. Investigaciones posteriores han explorado qué partes de estos ejemplos son cruciales, sugiriendo que la estructura del texto o el formato pueden ser más importantes que la etiqueta correcta para que el modelo aprenda la tarea \parencite{minRethinkingRoleDemonstrations2022}.

Diversas estrategias se centran en guiar el comportamiento del modelo. El \textit{role-prompting} \parencite{kongBetterZeroShotReasoning2024}, que asigna un rol específico al LLM (e.g., actuar como un experto), y el \textit{style prompting} \parencite{luBoundingCapabilitiesLarge2023}, que especifica el estilo deseado, permiten adaptar la respuesta a necesidades concretas. Para mejorar la calidad en tareas complejas, se ha aprovechado la capacidad de los LLM para inducirlos a generar pasos intermedios de razonamiento, un enfoque conocido como \textit{cadena de pensamiento} (\textit{Chain-of-Thought}) \parencite{nyeShowYourWork2021, weiChainofThoughtPromptingElicits2023}. Este paradigma se ha extendido incluso a escenarios sin ejemplos (\textit{zero-shot CoT}), donde se instruye al modelo a \textit{pensar paso a paso} \parencite{kojimaLargeLanguageModels2023, wangPlanandSolvePromptingImproving2023}.

Otras técnicas se enfocan en la descomposición y la consistencia. El \textit{complexity-based prompting} aprovecha que respuestas más complejas a menudo correlacionan con una mayor probabilidad de acierto \parencite{fuComplexityBasedPromptingMultiStep2023}, mientras que la descomposición de problemas (\textit{least-to-most prompting}) resuelve sub-tareas más simples para integrar sus respuestas en la solución final \parencite{zhouLeasttoMostPromptingEnables2023}. La consistencia también se ha utilizado como criterio, generando múltiples respuestas y eligiendo la más frecuente (\textit{self-consistency}) \parencite{wangSelfConsistencyImprovesChain2023}. Para problemas que requieren planificación, estructuras como el \textit{Tree of Thoughts} y el \textit{Graph of Thoughts} permiten al LLM explorar diferentes caminos de razonamiento \parencite{yaoTreeThoughtsDeliberate2023, bestaGraphThoughtsSolving2024}.

Finalmente, se han desarrollado enfoques de auto-mejora y meta-razonamiento. Técnicas como \textit{Self-Ask} incitan al LLM a generar y responder preguntas de seguimiento para clarificar la tarea \parencite{pressMeasuringNarrowingCompositionality2023}, mientras que \textit{Self-Refine} establece un marco iterativo donde el modelo critica y mejora sus propias respuestas \parencite{madaanSelfRefineIterativeRefinement2023}. El \textit{step-back prompting} instruye al modelo a resolver primero una versión más abstracta del problema para simplificar la tarea \parencite{zhengTakeStepBack2024}. Otros enfoques exploran la inducción de un análisis previo del modo de proceder (\textit{meta-reasoning}) \parencite{gaoMetaReasoningLarge2024}, el uso de representaciones cognitivas como el \textit{Sketch of Thought} \parencite{aytesSketchofThoughtEfficientLLM2025} o la generación y evaluación iterativa de pasos de razonamiento (\textit{Cumulative Reasoning}) \parencite{zhangCumulativeReasoningLarge2025}.

\subsection{Optimización de Contexto}
\label{subsec:context_optimization}
A pesar de que los \textit{LLM} modernos poseen ventanas de contexto nominalmente muy grandes, su capacidad para utilizar eficazmente toda esa información es limitada, un fenómeno conocido como \textit{lost in the middle} \parencite{liuLostMiddleHow2024}. La optimización de contexto busca mitigar este problema presentando al modelo únicamente la información más relevante y densa para una tarea específica.

La principal estrategia para ello es la generación aumentada por recuperación (\textit{Retrieval-Augmented Generation} o \textit{RAG}), que combina la memoria paramétrica del \textit{LLM} con una memoria no paramétrica externa \parencite{lewisRetrievalAugmentedGenerationKnowledgeIntensive2021}. En lugar de procesar un documento extenso, un sistema \textit{RAG} primero recupera los fragmentos de texto más relevantes de una base de conocimientos (e.g., \textit{Wikipedia}) y luego los proporciona como contexto al LLM para que genere la respuesta final. La expansión de la ventana de contexto en modelos como \textit{Gemini 1.5} y \textit{Qwen2.5-1M} ha hecho que el aprendizaje con muchos ejemplos (\textit{many-shot in-context learning}) se vuelva una estrategia viable y potente, donde se pueden incluir numerosos ejemplos relevantes directamente en el \textit{prompt} \parencite{teamGemini15Unlocking2024, yangQwen251MTechnicalReport2025, agarwalManyShotInContextLearning2024}.

La efectividad de \textit{RAG} depende críticamente de la granularidad de la información recuperada. En lugar de recuperar pasajes de longitud fija, se ha demostrado que el uso de unidades más finas como las \textit{proposiciones} —unidades de texto atómicas y autocontenidas— mejora la densidad de la información relevante, reduce el ruido y aumenta el rendimiento en tareas posteriores \parencite{chenDenseRetrievalWhat2024}.

Más allá de la recuperación, han surgido marcos de compresión y razonamiento agentivo. \textbf{QwenLong-CPRS} es un sistema que, mediante instrucciones en lenguaje natural, comprime dinámicamente un contexto extenso en un resumen optimizado y específico para la consulta, actuando como un intermediario inteligente antes de pasar la información al \textit{LLM} final \parencite{shenQwenLongCPRS$infty$LLMsDynamic2025}. Por otro lado, la \textit{Cadena de Clarificaciones} (\textit{Chain-of-Clarifications}) propone un flujo de trabajo en el que el modelo se enseña a sí mismo generando preguntas de clarificación sobre la consulta original, recuperando evidencia para responderlas y refinando iterativamente su comprensión antes de dar la respuesta definitiva \parencite{zhuangSelfTaughtAgenticLong2025}. Finalmente, los \textit{Recitation-Augmented LLMs} se centran en mejorar la capacidad del modelo para recordar y citar fielmente la información del contexto proporcionado, ajustándolo para que aprenda a copiar explícitamente los segmentos relevantes, lo que mejora la fiabilidad de las respuestas basadas en el texto \parencite{sunRecitationAugmentedLanguageModels2023}.

\subsection{Agentes Basados en \textit{LLM}}
\label{subsec:agentes_llm}
El verdadero potencial de los \textit{LLMs} reside en su capacidad para actuar de forma autónoma y utilizar herramientas externas para superar sus limitaciones inherentes, como en cálculos matemáticos, razonamiento complejo o la verificación de hechos. A medida que los \textit{LLM} han mejorado, investigadores y empresas han explorado cómo permitirles interactuar con sistemas externos.

El sistema \textit{MRKL} (\textit{Modular Reasoning, Knowledge, and Language}) \parencite{karpasMRKLSystemsModular2022} es una de las formulaciones más simples de un agente, utilizando un LLM como \textit{router} para acceder a múltiples herramientas (e.g., obtener el clima o la fecha actual) y combinar la información para generar una respuesta final. Modelos como \textit{PAL (Program-aided Language Model)} \parencite{gaoPALProgramaidedLanguage2023} traducen problemas directamente a código ejecutable, mientras que \textit{ToRA (Tool-Integrated Reasoning Agent)} \parencite{gouToRAToolIntegratedReasoning2024} intercala pasos de código y razonamiento. El paradigma \textit{ReAct (Reasoning and Acting)} \parencite{yaoReActSynergizingReasoning2023} permite a los agentes generar un pensamiento, tomar una acción y recibir una observación, manteniendo un historial de estos pasos para informar decisiones futuras. \textit{Reflexion} \parencite{shinnReflexionLanguageAgents2023} extiende \textit{ReAct} incorporando retroalimentación lingüística para refinar el comportamiento del agente. (\textit{RAG}) se considera un sistema de agente cuando la propia recuperación se trata como una herramienta externa.

Para organizar la creciente diversidad de agentes y establecer un marco conceptual coherente, se han propuesto las \textit{Arquitecturas Cognitivas para Agentes de Lenguaje} (\textit{CoALA}, por sus siglas en inglés) \parencite{sumersCognitiveArchitecturesLanguage2024}. Inspirado en la ciencia cognitiva y la inteligencia artificial simbólica, \textit{CoALA} describe a los agentes de lenguaje en función de tres componentes clave: una memoria modular (de trabajo, episódica, semántica y procedimental), un espacio de acciones estructurado (acciones internas como razonar o aprender, y acciones externas para interactuar con el entorno) y un proceso de toma de decisiones generalizado. Este marco no solo permite clasificar y comparar los sistemas existentes, como \textit{ReAct} o \textit{Reflexion}, sino que también proporciona un plano para diseñar agentes futuros más capaces y estructurados, contextualizando los desarrollos actuales dentro de la historia más amplia de la IA.

\section{Extracción de Conocimiento y Enlaces}
\label{sec:extraccion_conocimiento_enlaces}
La extracción de conocimiento y la creación de enlaces entre piezas de información son tareas fundamentales en la construcción de bases de conocimiento robustas y útiles. Tradicionalmente, estos procesos han sido manuales y laboriosos, pero los avances recientes en \textit{LLMs} han abierto nuevas vías para su automatización y enriquecimiento.

\subsection{Extracción de Conocimiento}
La extracción de conocimiento, en el contexto de la gestión del conocimiento personal, implica identificar y capturar las ideas clave, entidades y relaciones presentes en diversas fuentes de información. Los \textit{LLM} han demostrado una notable capacidad para esta tarea, abordándola desde diversas perspectivas metodológicas.

Un enfoque fundamental es la \textit{extracción de conocimiento abierto}, que busca generar representaciones lógicas a partir de texto sin restricciones de dominio. Investigaciones pioneras como la de \parencite{vandurmeOpenKnowledgeExtraction2008} utilizan el procesamiento composicional del lenguaje para derivar proposiciones estructuradas, mientras que trabajos más recientes como \parencite{songOpenFactFactualityEnhanced2023} se centran en la veracidad y expresividad de los tripletes extraídos, utilizando marcos semánticos y enlaces a \textit{Wikidata} para garantizar la calidad. A escala industrial, \parencite{qianOpenDomainKnowledge2023} presenta un \textit{framework} para extraer conocimiento de la web abierta, abordando desafíos de volumen, variedad y veracidad.

Para estructurar este proceso, se han propuesto diversos \textit{frameworks} y \textit{pipelines} basados en \textit{LLM}. El trabajo de \parencite{zhangExtractDefineCanonicalize2024} introduce un marco de tres fases (Extraer-Definir-Canonizar) que realiza una extracción abierta seguida de una definición y canonización de esquemas \textit{post-hoc}, permitiendo una gran flexibilidad. De manera similar, \parencite{kommineniHumanExpertsMachines2024} propone un \textit{pipeline} semi-automatizado que abarca todo el proceso de construcción de grafos de conocimiento, desde la generación de preguntas de competencia (\textit{CQs}) hasta la creación de la ontología y su poblamiento, utilizando un '\textit{LLM} juez' para la evaluación. Un paradigma innovador es el de \parencite{liKnowCoderCodingStructured2024}, que representa los esquemas de conocimiento como clases de Python, un formato que los \textit{LLM} comprenden de forma nativa, y emplea un aprendizaje en dos fases para la comprensión y seguimiento del esquema. Por otro lado, \parencite{luoOneKEDockerizedSchemaGuided2025} diseña un sistema multi-agente (\textit{Schema, Extraction, Reflection}) para guiar la extracción de conocimiento en diversos dominios y formatos de datos.

La aplicación de estas técnicas a dominios específicos y la extracción guiada es otra área clave. Para el dominio clínico, \parencite{liAutomatedClinicalData2024} utiliza \textit{LLM} condicionados con conocimiento externo mediante aprendizaje en contexto (\textit{in-context learning}) para extraer información de informes médicos. Para garantizar la coherencia con bases de conocimiento existentes, enfoques como los de \parencite{fengOntologygroundedAutomaticKnowledge2024} y \parencite{mccuskerLOKELinkedOpen2023} utilizan ontologías preexistentes (como la de \textit{Wikidata}) para guiar y fundamentar el proceso de extracción del \textit{LLM}. Finalmente, la interacción con el usuario se explora en \parencite{abolhasaniLeveragingLLMAutomated2024}, que presenta un \textit{pipeline} interactivo guiado por un algoritmo adaptativo de Cadena de Pensamiento (\textit{CoT}) para alinear la extracción de ontologías con los requisitos del usuario.

\subsection{Creación de Enlaces}
La creación de enlaces se refiere al establecimiento de conexiones significativas entre las piezas de conocimiento extraídas. Los \textit{LLM} no solo extraen, sino que también pueden inferir y predecir estas relaciones.

Un paradigma dominante consiste en utilizar los \textit{LLM} para la predicción de enlaces como una tarea de inferencia. En este sentido, \parencite{shuKnowledgeGraphLarge2024} convierte la estructura del grafo en \textit{prompts} de lenguaje natural para afinar \textit{LLM} en la tarea de predicción de enlaces multi-salto. De forma similar, \parencite{heLinkGPTTeachingLarge2024} entrena un \textit{LLM} de extremo a extremo que integra información estructural pareada mediante un ajuste de instrucciones en dos etapas. Para abordar la escalabilidad en grafos de gran tamaño, \parencite{biLPNLScalableLink2024} propone un \textit{pipeline} de muestreo en dos etapas y una estrategia de 'divide y vencerás' para gestionar la sobrecarga de información en los \textit{prompts}.

Una estrategia diferente consiste en la creación de enlaces sin supervisión o con pocos ejemplos. El trabajo de \parencite{cartaIterativeZeroShotLLM2023} destaca en este aspecto, utilizando un \textit{pipeline} de \textit{prompting} iterativo en modo \textit{zero-shot}, que no requiere ejemplos ni recursos externos para construir el grafo de conocimiento.

Finalmente, otra línea de investigación utiliza los \textit{LLM} como enriquecedores de características para modelos existentes. En lugar de predecir el enlace directamente, \parencite{parkEnhancingFutureLink2024} emplea un \textit{LLM} para generar descripciones ricas de los nodos, que luego se utilizan como características iniciales para mejorar el rendimiento de los modelos de Redes Neuronales de Grafo (\textit{GNN}) en la predicción de enlaces, especialmente en escenarios de arranque en frío (\textit{cold-start}).

La combinación de estas dos capacidades —extracción y enlace— tiene el potencial de transformar la construcción de bases de conocimiento. Trabajos como los de \parencite{zhuLLMsKnowledgeGraph2024} y \parencite{machadoLLMStoreLeveraging2024} exploran las capacidades generales de los \textit{LLM} para estas tareas, el primero concluyendo que son mejores asistentes de inferencia que extractores, y el segundo implementando un \textit{LLM} como un 'almacén' de conocimiento dinámico que sintetiza respuestas estructuradas al momento de la consulta. La investigación continúa abordando desafíos como la interpretabilidad, la escalabilidad y la superación de las limitaciones inherentes de los \textit{LLM} mediante arquitecturas de agentes y la integración de herramientas externas.

\section{\textit{LLM} en Aplicaciones de Toma de Notas}
\label{sec:integracion_llm_pkm}
Los avances en los \textit{LLMs} han catalizado una diversidad de herramientas para automatizar y enriquecer la toma de notas y la gestión del conocimiento personal (\textit{PKM}). Estas varían desde aplicaciones web hasta \textit{plugins} para plataformas consolidadas, enfocándose en distintos casos de uso.

Un área prominente es la \textit{asistencia conversacional y la respuesta a preguntas contextualizadas}. En \textit{Logseq}, \textit{plugins} como \textit{Logseq Copilot} permiten interactuar con la IA, indexando las notas del usuario para ofrecer respuestas basadas en el propio conocimiento. En \textit{Obsidian.md}, herramientas como \textit{BMO Chatbot for Obsidian} y \textit{ChatGPT MD} ofrecen interfaces de chat versátiles, conectándose a servicios en la nube y \textit{LLMs} locales, y permitiendo referenciar la nota actual o notas enlazadas en las conversaciones. \textit{Caret Obsidian Plugin} extiende esta funcionalidad al \textit{Canvas} para un chat no lineal. \textit{Notion AI} también integra de forma nativa la capacidad de responder preguntas sobre el espacio de trabajo.

La \textit{generación y mejora de contenido} es otro caso de uso fundamental. Herramientas como \textit{LLM Summary} en \textit{Obsidian} automatizan la creación de resúmenes de archivos \textit{PDF} y la extracción de conceptos clave. Para la creación de material de estudio, \textit{plugins} como \textit{Obsidian Flashcards LLM} y \textit{Quiz Generator} facilitan la generación de tarjetas de estudio y cuestionarios. En Logseq, \textit{ollama-logseq} y \textit{logseq-rag} permiten generar resúmenes y tarjetas de estudio. La mejora de la escritura y la reestructuración de notas se abordan con \textit{Zettelkasten LLM Tools} y \textit{Simple Prompt} en Obsidian, mientras que \textit{LaTeX Generator Plugin for Obsidian} convierte lenguaje natural a ecuaciones \textit{LaTeX}. Para la generación de contenido dentro de lienzos visuales, \textit{Canvas LLM Extender} en Obsidian añade nodos de texto generados por IA.

En cuanto a la \textit{organización, estructuración y enlace del conocimiento}, \textit{InfraNodus} destaca como una aplicación web que visualiza texto como redes para identificar términos influyentes y lagunas conceptuales. En Obsidian, \textit{ExMemo Tools} se enfoca en la gestión de documentos y la generación de metadatos como etiquetas, una tarea que \textit{Obsidian LLM Tagger Plugin} también realiza usando \textit{LLMs} locales. \textit{InsightA Obsidian Plugin} transforma artículos largos en notas atómicas interconectadas y genera Mapas de Contenido (\textit{MOCs}). \textit{Obsidian Cloud Atlas Plugin} utiliza reconocimiento de entidades para crear \textit{wikilinks} automáticamente, mejorando la interconexión.

La integración con \textit{LLMs} locales para mayor privacidad y control es una tendencia creciente. En Logseq, \textit{ollama-logseq} y \textit{logseq-rag} se integran con \textit{Ollama}. Para Obsidian, \textit{Local LLM Helper - Obsidian Plugin} y \textit{Obsidian AI plugin} conectan con servidores locales como \textit{Ollama} o \textit{LM Studio}. Muchas de las herramientas de chat y generación de contenido mencionadas también ofrecen la opción de operar con \textit{LLMs} locales.

Finalmente, la \textit{automatización de flujos de trabajo} y el \textit{RAG} permiten interacciones más sofisticadas. \textit{Logseq Composer} conecta notas con cualquier \textit{LLM} mediante \textit{RAG} a través de \textit{LiteLLM}. En \textit{Obsidian}, \textit{Cannoli} permite construir \textit{scripts} de \textit{LLM} sin código en el editor \textit{Canvas}, y \textit{LLM Workspace plugin for Obsidian} permite crear conjuntos de fuentes curadas para fundamentar las conversaciones con IA mediante \textit{RAG}. \textit{Obsidian Cloud Atlas Plugin} también introduce flujos de trabajo en \textit{Canvas} o \textit{Markdown}.

Esta evolución subraya un esfuerzo por automatizar tareas, mejorar la interconexión de ideas y potenciar la utilidad de los sistemas de \textit{PKM}, con una diversidad de enfoques que buscan adaptarse a las necesidades específicas de los usuarios.

\section{Fundamentos y Alcance de la Investigación}
\label{sec:fundamentos_investigacion}
El presente trabajo de tesis se enmarca en la intersección de la Gestión del Conocimiento Personal (PKM) y los avances en Inteligencia Artificial, específicamente los \textit{LLM}. La investigación busca abordar los desafíos inherentes a la integración eficiente y significativa de nuevo conocimiento en bases de conocimiento personales semiestructuradas.

El problema científico que se aborda es la optimización del proceso de integración de conocimiento en dichas bases. Tradicionalmente, este proceso es manual y consume tiempo, especialmente al incorporar información de diversas fuentes y formatos, y al establecer conexiones relevantes. Se busca explorar cómo los \textit{LLM} pueden automatizar y enriquecer esta tarea, facilitando la asimilación de información y la adaptación a diferentes paradigmas de toma de notas (e.g., \textit{Zettelkasten}, notas conectadas, resúmenes progresivos). El objeto de estudio es, por tanto, el proceso de construcción y enriquecimiento automatizado de bases de conocimiento personal mediante LLM, centrándose el campo de acción en sistemas basados en lenguajes de marcado como \textit{Markdown} y la representación del conocimiento mediante grafos, con especial atención a los \textit{trabajadores del conocimiento}.

La investigación se guía por la pregunta: ¿En qué medida la aplicación de \textit{LLMs}, a través de un \textit{framework} de agentes personalizables, puede automatizar y optimizar la integración de conocimiento proveniente de diversas fuentes (predominantemente no estructuradas) en bases de conocimiento personales semiestructuradas (como las basadas en \textit{Markdown} y grafos), y cómo se adapta esta automatización a diferentes paradigmas de toma de notas?

Para responder a esta pregunta, el \textit{Objetivo General} es avanzar en la automatización de la construcción incremental y progresiva de bases de conocimiento personal, con un enfoque en la integración contextualizada de información mediante \textit{LLM}. Los \textit{Objetivos Específicos} son:
\begin{enumerate}
    \item Desarrollar un \textit{framework} flexible capaz de procesar datos en múltiples formatos (e.g., texto, \textit{PDF}, web), predominantemente no estructurados, para su integración en bases de conocimiento personales existentes (e.g., \textit{Obsidian.md}).
    \item Diseñar e implementar un conjunto de agentes basados en \textit{LLM} con distintas capacidades (resumen, extracción de entidades, generación de enlaces, creación de notas atómicas) para la integración de conocimiento.
    \item Evaluar la eficacia y adaptabilidad del sistema para soportar diversos paradigmas de toma de notas, analizando la coherencia, relevancia y utilidad del conocimiento integrado.
\end{enumerate}

Este esfuerzo continúa la línea de investigación de trabajos como \cite{fragaAutomaticGenerationKnowledge2023}, pero se enfoca en el aprovechamiento de LLM para generar y enriquecer información directamente en el contexto de una base de conocimiento personal existente o en desarrollo, particularmente para la creación y expansión de notas semiestructuradas en \textit{Markdown} que forman un grafo de conocimiento dinámico. Hasta donde alcanza el conocimiento del autor, no existe una herramienta que combine integralmente \textit{LLM} para la construcción progresiva y contextualizada de una base de conocimiento personal basada en notas interconectadas, considerando la diversidad de fuentes y paradigmas. Para abordar esta brecha, se ha desarrollado un prototipo que procesa archivos, interactúa con un \textit{LLM} para extraer, sintetizar e integrar conocimiento en una base de notas (existente o nueva), generando nuevas notas, resúmenes, conexiones y metadatos. Este trabajo explorará la viabilidad, eficacia y desafíos de dicha automatización, buscando una contribución significativa a la \textit{PKM} y a las aplicaciones prácticas de los \textit{LLM}.