@misc{agarwalManyShotInContextLearning2024,
  title = {Many-{{Shot In-Context Learning}}},
  author = {Agarwal, Rishabh and Singh, Avi and Zhang, Lei M. and Bohnet, Bernd and Rosias, Luis and Chan, Stephanie and Zhang, Biao and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and {Co-Reyes}, John D. and Chu, Eric and Behbahani, Feryal and Faust, Aleksandra and Larochelle, Hugo},
  year = {2024},
  month = oct,
  number = {arXiv:2404.11018},
  eprint = {2404.11018},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.11018},
  urldate = {2025-02-28},
  abstract = {Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. We also find that inference cost increases linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL to varying degrees. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\6MSGDPAW\Agarwal et al. - 2024 - Many-Shot In-Context Learning.pdf}
}

@misc{aytesSketchofThoughtEfficientLLM2025,
  title = {Sketch-of-{{Thought}}: {{Efficient LLM Reasoning}} with {{Adaptive Cognitive-Inspired Sketching}}},
  shorttitle = {Sketch-of-{{Thought}}},
  author = {Aytes, Simon A. and Baek, Jinheon and Hwang, Sung Ju},
  year = {2025},
  month = mar,
  number = {arXiv:2503.05179},
  eprint = {2503.05179},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.05179},
  urldate = {2025-03-22},
  abstract = {Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketchof-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms---Conceptual Chaining, Chunked Symbolism, and Expert Lexicons---each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76\% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\QJSH8R42\Aytes et al. - 2025 - Sketch-of-Thought Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching.pdf}
}

@article{bestaGraphThoughtsSolving2024,
  title = {Graph of {{Thoughts}}: {{Solving Elaborate Problems}} with {{Large Language Models}}},
  shorttitle = {Graph of {{Thoughts}}},
  author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  eprint = {2308.09687},
  primaryclass = {cs},
  pages = {17682--17690},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i16.29720},
  urldate = {2025-02-28},
  abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-ofThought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (``LLM thoughts'') are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {$>$}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\68WJR4TA\Besta et al. - 2024 - Graph of Thoughts Solving Elaborate Problems with Large Language Models.pdf}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = dec,
  series = {{{NIPS}} '20},
  pages = {1877--1901},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-05-01},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  isbn = {978-1-7138-2954-6},
  file = {C:\Users\zahdehv\Zotero\storage\W7X8GUHM\Brown et al. - 2020 - Language models are few-shot learners.pdf}
}

@inproceedings{chenDenseRetrievalWhat2024,
  title = {Dense {{X Retrieval}}: {{What Retrieval Granularity Should We Use}}?},
  shorttitle = {Dense {{X Retrieval}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Chen, Tong and Wang, Hongwei and Chen, Sihao and Yu, Wenhao and Ma, Kaixin and Zhao, Xinran and Zhang, Hongming and Yu, Dong},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {15159--15177},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.845},
  urldate = {2025-04-12},
  abstract = {Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.},
  file = {C:\Users\zahdehv\Zotero\storage\GBCDZAL6\Chen et al. - 2024 - Dense X Retrieval What Retrieval Granularity Should We Use.pdf}
}

@misc{chenReverseThinkingMakes2024,
  title = {Reverse {{Thinking Makes LLMs Stronger Reasoners}}},
  author = {Chen, Justin Chih-Yao and Wang, Zifeng and Palangi, Hamid and Han, Rujun and Ebrahimi, Sayna and Le, Long and Perot, Vincent and Mishra, Swaroop and Bansal, Mohit and Lee, Chen-Yu and Pfister, Tomas},
  year = {2024},
  month = nov,
  number = {arXiv:2411.19865},
  eprint = {2411.19865},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.19865},
  urldate = {2025-02-28},
  abstract = {Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (REVTHINK), a framework composed of data augmentation and learning objectives. In REVTHINK, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53\% improvement over the student model's zero-shot performance and a 6.84\% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency -- using only 10\% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10{\texttimes} more forward reasoning. REVTHINK also exhibits strong generalization to out-of-distribution held-out datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\UP9S6X29\Chen et al. - 2024 - Reverse Thinking Makes LLMs Stronger Reasoners.pdf}
}

@article{dominguezAcotacionEspacioBusqueda,
  title = {{Acotaci{\'o}n del espacio de b{\'u}squeda de la regresi{\'o}n simb{\'o}lica para ecuaciones diferenciales ordinarias lineales en los par{\'a}metros mediante redes neuronales}},
  author = {Dom{\'i}nguez, David Guaty},
  langid = {spanish},
  file = {C:\Users\zahdehv\Zotero\storage\AX69QEBK\Domínguez - Acotación del espacio de búsqueda de la regresión simbólica para ecuaciones diferenciales ordinarias.pdf}
}

@misc{fernandoPromptbreederSelfReferentialSelfImprovement2023,
  title = {Promptbreeder: {{Self-Referential Self-Improvement Via Prompt Evolution}}},
  shorttitle = {Promptbreeder},
  author = {Fernando, Chrisantha and Banarse, Dylan and Michalewski, Henryk and Osindero, Simon and Rockt{\"a}schel, Tim},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16797},
  eprint = {2309.16797},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.16797},
  urldate = {2025-05-01},
  abstract = {Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furthermore, Promptbreeder is able to evolve intricate task-prompts for the challenging problem of hate speech classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\VZTZCK3C\\Fernando et al. - 2023 - Promptbreeder Self-Referential Self-Improvement Via Prompt Evolution.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\9S63IK6Q\\2309.html}
}

@misc{fuComplexityBasedPromptingMultiStep2023,
  title = {Complexity-{{Based Prompting}} for {{Multi-Step Reasoning}}},
  author = {Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
  year = {2023},
  month = jan,
  number = {arXiv:2210.00720},
  eprint = {2210.00720},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.00720},
  urldate = {2025-05-01},
  abstract = {We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\G5SFYR8I\\Fu et al. - 2023 - Complexity-Based Prompting for Multi-Step Reasoning.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\E7CZVMYB\\2210.html}
}

@misc{gaoMetaReasoningLarge2024,
  title = {Meta {{Reasoning}} for {{Large Language Models}}},
  author = {Gao, Peizhong and Xie, Ao and Mao, Shaoguang and Wu, Wenshan and Xia, Yan and Mi, Haipeng and Wei, Furu},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11698},
  eprint = {2406.11698},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.11698},
  urldate = {2025-02-28},
  abstract = {We introduce Meta-Reasoning Prompting (MRP), a novel and efficient system prompting method for large language models (LLMs) inspired by human metareasoning. Traditional in-context learning-based reasoning techniques, such as Tree-of-Thoughts, show promise but lack consistent state-of-the-art performance across diverse tasks due to their specialized nature. MRP addresses this limitation by guiding LLMs to dynamically select and apply different reasoning methods based on the specific requirements of each task, optimizing both performance and computational efficiency. With MRP, LLM reasoning operates in two phases. Initially, the LLM identifies the most appropriate reasoning method using task input cues and objective descriptions of available methods. Subsequently, it applies the chosen method to complete the task. This dynamic strategy mirrors human meta-reasoning, allowing the model to excel in a wide range of problem domains. We evaluate the effectiveness of MRP through comprehensive benchmarks. The results demonstrate that MRP achieves or approaches state-of-the-art performance across diverse tasks. MRP represents a significant advancement in enabling LLMs to identify cognitive challenges across problems and leverage benefits across different reasoning approaches, enhancing their ability to handle diverse and complex problem domains efficiently. Every LLM deserves a Meta-Reasoning Prompting to unlock its full potential and ensure adaptability in an ever-evolving landscape of challenges and applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\zahdehv\Zotero\storage\B4UKAFK6\Gao et al. - 2024 - Meta Reasoning for Large Language Models.pdf}
}

@misc{gautamFactGeniusCombiningZeroShot2024,
  title = {{{FactGenius}}: {{Combining Zero-Shot Prompting}} and {{Fuzzy Relation Mining}} to {{Improve Fact Verification}} with {{Knowledge Graphs}}},
  shorttitle = {{{FactGenius}}},
  author = {Gautam, Sushant},
  year = {2024},
  month = jun,
  number = {arXiv:2406.01311},
  eprint = {2406.01311},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.01311},
  urldate = {2025-04-11},
  abstract = {Fact-checking is a crucial natural language processing (NLP) task that verifies the truthfulness of claims by considering reliable evidence. Traditional methods are often limited by labour-intensive data curation and rule-based approaches. In this paper, we present FactGenius, a novel method that enhances fact-checking by combining zero-shot prompting of large language models (LLMs) with fuzzy text matching on knowledge graphs (KGs). Leveraging DBpedia, a structured linked data dataset derived from Wikipedia, FactGenius refines LLM-generated connections using similarity measures to ensure accuracy. The evaluation of FactGenius on the FactKG, a benchmark dataset for fact verification, demonstrates that it significantly outperforms existing baselines, particularly when fine-tuning RoBERTa as a classifier. The two-stage approach of filtering and validating connections proves crucial, achieving superior performance across various reasoning types and establishing FactGenius as a promising tool for robust fact-checking. The code and materials are available at https://github.com/SushantGautam/FactGenius.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\S6SBXJMH\\Gautam - 2024 - FactGenius Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification wit.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\GNFPHMCN\\2406.html}
}

@misc{hewingPromptCanvasLiteratureBased2024,
  title = {The {{Prompt Canvas}}: {{A Literature-Based Practitioner Guide}} for {{Creating Effective Prompts}} in {{Large Language Models}}},
  shorttitle = {The {{Prompt Canvas}}},
  author = {Hewing, Michael and Leinhos, Vincent},
  year = {2024},
  month = dec,
  number = {arXiv:2412.05127},
  eprint = {2412.05127},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.05127},
  urldate = {2025-02-28},
  abstract = {The rise of large language models (LLMs) has highlighted the importance of prompt engineering as a crucial technique for optimizing model outputs. While experimentation with various prompting methods, such as Few-shot, Chain-of-Thought, and role-based techniques, has yielded promising results, these advancements remain fragmented across academic papers, blog posts and anecdotal experimentation. The lack of a single, unified resource to consolidate the field's knowledge impedes the progress of both research and practical application. This paper argues for the creation of an overarching framework that synthesizes existing methodologies into a cohesive overview for practitioners. Using a design-based research approach, we present the Prompt Canvas (Figure 1), a structured framework resulting from an extensive literature review on prompt engineering that captures current knowledge and expertise. By combining the conceptual foundations and practical strategies identified in prompt engineering, the Prompt Canvas provides a practical approach for leveraging the potential of Large Language Models. It is primarily designed as a learning resource for pupils, students and employees, offering a structured introduction to prompt engineering. This work aims to contribute to the growing discourse on prompt engineering by establishing a unified methodology for researchers and providing guidance for practitioners.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C:\Users\zahdehv\Zotero\storage\GVRBUX3S\Hewing and Leinhos - 2024 - The Prompt Canvas A Literature-Based Practitioner Guide for Creating Effective Prompts in Large Lan.pdf}
}

@misc{ingalaApproximationAlgorithmsRectangle2017,
  title = {Approximation {{Algorithms}} for {{Rectangle Packing Problems}} ({{PhD Thesis}})},
  author = {Ingala, Salvatore},
  year = {2017},
  month = nov,
  number = {arXiv:1711.07851},
  eprint = {1711.07851},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.07851},
  urldate = {2025-02-28},
  abstract = {In rectangle packing problems we are given the task of placing axis-aligned rectangles in a given plane region, so that they do not overlap with each other. In Maximum Weight Independent Set of Rectangles (MWISR), their position is given and we can only select which rectangles to choose, while trying to maximize their total weight. In Strip Packing (SP), we have to pack all the given rectangles in a rectangular region of fixed width, while minimizing its height. In 2-Dimensional Geometric Knapsack (2DGK), the target region is a square of a given size, and our goal is to select and pack a subset of the given rectangles of maximum weight. We study a generalization of MWISR and use it to improve the approximation for a resource allocation problem called bagUFP. We revisit some classical results on SP and 2DGK, by proposing a framework based on smaller containers that are packed with simpler rules; while variations of this scheme are indeed a standard technique in this area, we abstract away some of the problem-specific differences, obtaining simpler algorithms that work for different problems. We obtain improved approximations for SP in pseudo-polynomial time, and for a variant of 2DGK where one can to rotate the rectangles by 90\{{\textbackslash}deg\}. For the latter, we propose the first algorithms with approximation factor better than 2. For the main variant of 2DGK (without rotations), a container-based approach seems to face a natural barrier of 2 in the approximation factor. Thus, we consider a generalized kind of packing that combines container packings with another packing problem that we call L-packing problem, where we have to pack rectangles in an L-shaped region of the plane. By finding a (1 + \{{\textbackslash}epsilon\})-approximation for this problem and exploiting the combinatorial structure of 2DGK, we obtain the first algorithms that break the barrier of 2 for the approximation factor of this problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {C:\Users\zahdehv\Zotero\storage\ED86ALGF\Ingala - 2017 - Approximation Algorithms for Rectangle Packing Problems (PhD Thesis).pdf}
}

@inproceedings{jungMaieuticPromptingLogically2022,
  title = {Maieutic {{Prompting}}: {{Logically Consistent Reasoning}} with {{Recursive Explanations}}},
  shorttitle = {Maieutic {{Prompting}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Le Bras, Ronan and Choi, Yejin},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {1266--1279},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.82},
  urldate = {2025-05-01},
  abstract = {Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20\% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.},
  file = {C:\Users\zahdehv\Zotero\storage\MYWWDH32\Jung et al. - 2022 - Maieutic Prompting Logically Consistent Reasoning with Recursive Explanations.pdf}
}

@misc{karpasMRKLSystemsModular2022,
  title = {{{MRKL Systems}}: {{A}} Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning},
  shorttitle = {{{MRKL Systems}}},
  author = {Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and {Leyton-Brown}, Kevin and Muhlgay, Dor and Rozen, Noam and Schwartz, Erez and Shachaf, Gal and {Shalev-Shwartz}, Shai and Shashua, Amnon and Tenenholtz, Moshe},
  year = {2022},
  month = may,
  number = {arXiv:2205.00445},
  eprint = {2205.00445},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.00445},
  urldate = {2025-05-01},
  abstract = {Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\HB58GB8B\\Karpas et al. - 2022 - MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external k.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\JEZYEM45\\2205.html}
}

@misc{kojimaLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2023},
  month = jan,
  number = {arXiv:2205.11916},
  eprint = {2205.11916},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.11916},
  urldate = {2025-05-02},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\VHKC25TU\\Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\V6ANAGHX\\2205.html}
}

@misc{longLargeLanguageModel2023,
  title = {Large {{Language Model Guided Tree-of-Thought}}},
  author = {Long, Jieyi},
  year = {2023},
  month = may,
  number = {arXiv:2305.08291},
  eprint = {2305.08291},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.08291},
  urldate = {2025-02-28},
  abstract = {In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: https://github.com/jieyilong/tree-of-thought-puzzle-solver.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\zahdehv\Zotero\storage\VICY279N\Long - 2023 - Large Language Model Guided Tree-of-Thought.pdf}
}

@misc{lotfiVisualScratchpadsEnabling2024,
  title = {Visual {{Scratchpads}}: {{Enabling Global Reasoning}} in {{Vision}}},
  shorttitle = {Visual {{Scratchpads}}},
  author = {Lotfi, Aryo and Fini, Enrico and Bengio, Samy and Nabi, Moin and Abbe, Emmanuel},
  year = {2024},
  month = oct,
  number = {arXiv:2410.08165},
  eprint = {2410.08165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.08165},
  urldate = {2025-04-04},
  abstract = {Modern vision models have achieved remarkable success in benchmarks where local features provide critical information about the target. There is now a growing interest in solving tasks that require more global reasoning, where local features offer no significant information. These tasks are reminiscent of the connectivity tasks discussed by Minsky and Papert in 1969, which exposed the limitations of the perceptron model and contributed to the first AI winter. In this paper, we revisit such tasks by introducing four global visual benchmarks involving path findings and mazes. We show that: (1) although today's large vision models largely surpass the expressivity limitations of the early models, they still struggle with the learning efficiency; we put forward the "globality degree" notion to understand this limitation; (2) we then demonstrate that the picture changes and global reasoning becomes feasible with the introduction of "visual scratchpads"; similarly to the text scratchpads and chain-of-thoughts used in language models, visual scratchpads help break down global tasks into simpler ones; (3) we finally show that some scratchpads are better than others, in particular, "inductive scratchpads" that take steps relying on less information afford better out-of-distribution generalization and succeed for smaller model sizes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\Z7BDZKHU\\Lotfi et al. - 2024 - Visual Scratchpads Enabling Global Reasoning in Vision.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\LPRDLHSE\\2410.html}
}

@misc{lyuFaithfulChainofThoughtReasoning2023,
  title = {Faithful {{Chain-of-Thought Reasoning}}},
  author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and {Callison-Burch}, Chris},
  year = {2023},
  month = sep,
  number = {arXiv:2301.13379},
  eprint = {2301.13379},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.13379},
  urldate = {2025-02-28},
  abstract = {While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query \${\textbackslash}rightarrow\$ symbolic reasoning chain) and Problem Solving (reasoning chain \${\textbackslash}rightarrow\$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3\% on Math Word Problems (MWP), 3.4\% on Planning, 5.5\% on Multi-hop Question Answering (QA), and 21.4\% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\zahdehv\Zotero\storage\SDT2CES7\Lyu et al. - 2023 - Faithful Chain-of-Thought Reasoning.pdf}
}

@misc{madaanSelfRefineIterativeRefinement2023,
  title = {Self-{{Refine}}: {{Iterative Refinement}} with {{Self-Feedback}}},
  shorttitle = {Self-{{Refine}}},
  author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
  year = {2023},
  month = may,
  number = {arXiv:2303.17651},
  eprint = {2303.17651},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17651},
  urldate = {2025-05-01},
  abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\4Z67JCMG\\Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedback.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\5X3LCKDX\\2303.html}
}

@misc{metropolitanskyEffectiveExtractionEvaluation2025,
  title = {Towards {{Effective Extraction}} and {{Evaluation}} of {{Factual Claims}}},
  author = {Metropolitansky, Dasha and Larson, Jonathan},
  year = {2025},
  month = feb,
  number = {arXiv:2502.10855},
  eprint = {2502.10855},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.10855},
  urldate = {2025-03-28},
  abstract = {A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\UIZ8CI9C\\Metropolitansky and Larson - 2025 - Towards Effective Extraction and Evaluation of Factual Claims.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\8HRKURKH\\2502.html}
}

@article{millerMagicalNumberSeven1956,
  title = {The {{Magical Number Seven}}, {{Plus}} or {{Minus Two}}: {{Some Limits}} on Our {{Capacity}} for {{Processing Information}}[1]},
  author = {Miller, George A},
  year = {1956},
  langid = {english},
  file = {C:\Users\zahdehv\Zotero\storage\G46UYFMY\Miller - The Magical Number Seven, Plus or Minus Two Some Limits on our Capacity for Processing Information[.pdf}
}

@inproceedings{minRethinkingRoleDemonstrations2022,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {11048--11064},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.759},
  urldate = {2025-05-01},
  abstract = {Large language models (LMs) are able to in-context learn---perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required---randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  file = {C:\Users\zahdehv\Zotero\storage\R682XKKR\Min et al. - 2022 - Rethinking the Role of Demonstrations What Makes In-Context Learning Work.pdf}
}

@inproceedings{mishraReframingInstructionalPrompts2022,
  title = {Reframing {{Instructional Prompts}} to {{GPTk}}`s {{Language}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Choi, Yejin and Hajishirzi, Hannaneh},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = {2022},
  month = may,
  pages = {589--612},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.50},
  urldate = {2025-05-01},
  abstract = {What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5\% and 6.7\% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.},
  file = {C:\Users\zahdehv\Zotero\storage\T29UEPG8\Mishra et al. - 2022 - Reframing Instructional Prompts to GPTk`s Language.pdf}
}

@misc{nyeShowYourWork2021,
  title = {Show {{Your Work}}: {{Scratchpads}} for {{Intermediate Computation}} with {{Language Models}}},
  shorttitle = {Show {{Your Work}}},
  author = {Nye, Maxwell and Andreassen, Anders Johan and {Gur-Ari}, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
  year = {2021},
  month = nov,
  number = {arXiv:2112.00114},
  eprint = {2112.00114},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.00114},
  urldate = {2025-04-04},
  abstract = {Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\GEKSUMX8\\Nye et al. - 2021 - Show Your Work Scratchpads for Intermediate Computation with Language Models.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\PBN2GP6L\\2112.html}
}

@misc{plaatReasoningLargeLanguage2024,
  title = {Reasoning with {{Large Language Models}}, a {{Survey}}},
  author = {Plaat, Aske and Wong, Annie and Verberne, Suzan and Broekens, Joost and van Stein, Niki and Back, Thomas},
  year = {2024},
  month = jul,
  number = {arXiv:2407.11511},
  eprint = {2407.11511},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.11511},
  urldate = {2025-02-28},
  abstract = {Scaling up language models to billions of parameters has opened up possibilities for in-context learning, allowing instruction tuning and few-shot learning on tasks that the model was not specifically trained for. This has achieved breakthrough performance on language tasks such as translation, summarization, and question-answering. Furthermore, in addition to these associative ``System 1'' tasks, recent advances in Chain-of-thought prompt learning have demonstrated strong ``System 2'' reasoning abilities, answering a question in the field of artificial general intelligence whether LLMs can reason.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\SVEX5JVF\Plaat et al. - 2024 - Reasoning with Large Language Models, a Survey.pdf}
}

@misc{schickToolformerLanguageModels2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04761},
  eprint = {2302.04761},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.04761},
  urldate = {2025-05-01},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\AT7P3SDS\\Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves to Use Tools.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\7JYEUBLB\\2302.html}
}

@misc{schmidgallAgentRxivCollaborativeAutonomous2025,
  title = {{{AgentRxiv}}: {{Towards Collaborative Autonomous Research}}},
  shorttitle = {{{AgentRxiv}}},
  author = {Schmidgall, Samuel and Moor, Michael},
  year = {2025},
  month = mar,
  number = {arXiv:2503.18102},
  eprint = {2503.18102},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.18102},
  urldate = {2025-04-25},
  abstract = {Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other's research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4\% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3\%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7\% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\BB3XA7SY\Schmidgall and Moor - 2025 - AgentRxiv Towards Collaborative Autonomous Research.pdf}
}

@misc{schulhoffLearnPromptingCited,
  title = {Learn {{Prompting}}: {{Cited Papers}} on {{AI}} and {{Prompt Engineering}}},
  shorttitle = {Learn {{Prompting}}},
  author = {Schulhoff", "Sander},
  urldate = {2025-05-01},
  abstract = {Explore an organized list of research papers cited in the Learn Prompting course, covering topics like agents, automated prompts, datasets, detection, and more.},
  howpublished = {https://learnprompting.org/docs/bibliography},
  langid = {english},
  file = {C:\Users\zahdehv\Zotero\storage\L3PV75AA\bibliography.html}
}

@misc{schulhoffPromptReportSystematic2025,
  title = {The {{Prompt Report}}: {{A Systematic Survey}} of {{Prompt Engineering Techniques}}},
  shorttitle = {The {{Prompt Report}}},
  author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Costa, Hevander Da and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
  year = {2025},
  month = feb,
  number = {arXiv:2406.06608},
  eprint = {2406.06608},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.06608},
  urldate = {2025-05-01},
  abstract = {Generative Artificial Intelligence (GenAI) systems are increasingly being deployed across diverse industries and research domains. Developers and end-users interact with these systems through the use of prompting and prompt engineering. Although prompt engineering is a widely adopted and extensively researched area, it suffers from conflicting terminology and a fragmented ontological understanding of what constitutes an effective prompt due to its relatively recent emergence. We establish a structured understanding of prompt engineering by assembling a taxonomy of prompting techniques and analyzing their applications. We present a detailed vocabulary of 33 vocabulary terms, a taxonomy of 58 LLM prompting techniques, and 40 techniques for other modalities. Additionally, we provide best practices and guidelines for prompt engineering, including advice for prompting state-of-the-art (SOTA) LLMs such as ChatGPT. We further present a meta-analysis of the entire literature on natural language prefix-prompting. As a culmination of these efforts, this paper presents the most comprehensive survey on prompt engineering to date.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\SL573B6J\\Schulhoff et al. - 2025 - The Prompt Report A Systematic Survey of Prompt Engineering Techniques.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\GJEBBKXL\\2406.html}
}

@article{shanahanRolePlayLarge2023,
  title = {Role Play with Large Language Models},
  author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  year = {2023},
  month = nov,
  journal = {Nature},
  volume = {623},
  number = {7987},
  pages = {493--498},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06647-8},
  abstract = {As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.},
  langid = {english},
  pmid = {37938776},
  keywords = {Deception,Humans,Imitative Behavior,Natural Language Processing,Self-Assessment,Terminology as Topic},
  file = {C:\Users\zahdehv\Zotero\storage\3MUCU3DJ\Shanahan et al. - 2023 - Role play with large language models.pdf}
}

@misc{sumersCognitiveArchitecturesLanguage2024,
  title = {Cognitive {{Architectures}} for {{Language Agents}}},
  author = {Sumers, Theodore R. and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
  year = {2024},
  month = mar,
  number = {arXiv:2309.02427},
  eprint = {2309.02427},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.02427},
  urldate = {2025-02-28},
  abstract = {Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decisionmaking process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  file = {C:\Users\zahdehv\Zotero\storage\MCSQ8LMK\Sumers et al. - 2024 - Cognitive Architectures for Language Agents.pdf}
}

@misc{sunRecitationAugmentedLanguageModels2023,
  title = {Recitation-{{Augmented Language Models}}},
  author = {Sun, Zhiqing and Wang, Xuezhi and Tay, Yi and Yang, Yiming and Zhou, Denny},
  year = {2023},
  month = feb,
  number = {arXiv:2210.01296},
  eprint = {2210.01296},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.01296},
  urldate = {2025-05-01},
  abstract = {We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of {\textbackslash}method{\textasciitilde}on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at "https://github.com/Edward-Sun/RECITE".},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\TAZSYTIP\\Sun et al. - 2023 - Recitation-Augmented Language Models.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\BPL8FF9Y\\2210.html}
}

@misc{sunTableThoughtExploring2025,
  title = {Table as {{Thought}}: {{Exploring Structured Thoughts}} in {{LLM Reasoning}}},
  shorttitle = {Table as {{Thought}}},
  author = {Sun, Zhenjie and Deng, Naihao and Yu, Haofei and You, Jiaxuan},
  year = {2025},
  month = jan,
  number = {arXiv:2501.02152},
  eprint = {2501.02152},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.02152},
  urldate = {2025-02-28},
  abstract = {Large language models' reasoning abilities benefit from methods that organize their thought processes, such as chain-of-thought prompting, which employs a sequential structure to guide the reasoning process step-by-step. However, existing approaches focus primarily on organizing the sequence of thoughts, leaving structure in individual thought steps underexplored. To address this gap, we propose Table as Thought, a framework inspired by cognitive neuroscience theories on human thought. Table as Thought organizes reasoning within a tabular schema, where rows represent sequential thought steps and columns capture critical constraints and contextual information to enhance reasoning. The reasoning process iteratively populates the table until self-verification ensures completeness and correctness. Our experiments show that Table as Thought excels in planning tasks and demonstrates a strong potential for enhancing LLM performance in mathematical reasoning compared to unstructured thought baselines. This work provides a novel exploration of refining thought representation within LLMs, paving the way for advancements in reasoning and AI cognition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\zahdehv\Zotero\storage\7NVXI49A\Sun et al. - 2025 - Table as Thought Exploring Structured Thoughts in LLM Reasoning.pdf}
}

@inproceedings{wangKBLaMKnowledgeBase2024,
  title = {{{KBLaM}}: {{Knowledge Base}} Augmented {{Language Model}}},
  shorttitle = {{{KBLaM}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Wang, Xi and Isazawa, Taketomo and Mikaelyan, Liana and Hensman, James},
  year = {2024},
  month = oct,
  urldate = {2025-03-20},
  abstract = {In this paper, we propose Knowledge Base augmented Language Model (KBLAM), a new method for augmenting Large Language Models (LLMs) with external knowledge. KBLAM works with a knowledge base (KB) constructed from a corpus of documents, transforming each piece of knowledge in the KB into continuous key-value vector pairs via pre-trained sentence encoders with linear adapters and integrating them into pre-trained LLMs via a specialized rectangular attention mechanism. Unlike Retrieval-Augmented Generation, KBLAM eliminates external retrieval modules, and unlike in-context learning, its computational overhead scales linearly with KB size rather than quadratically. Our approach enables integrating a large KB of more than 10K triples into an 8B pre-trained LLM of only 8K context window on one single A100 80GB GPU and allows for dynamic updates without model fine-tuning or retraining. Experiments demonstrate KBLAM's effectiveness in various tasks, including question-answering and open-ended reasoning, while providing interpretable insights into its use of the augmented knowledge. Code and datasets are available at https://github.com/microsoft/KBLaM/},
  langid = {english},
  file = {C:\Users\zahdehv\Zotero\storage\5N4UXN7A\Wang et al. - 2024 - KBLaM Knowledge Base augmented Language Model.pdf}
}

@misc{wangPlanandSolvePromptingImproving2023,
  title = {Plan-and-{{Solve Prompting}}: {{Improving Zero-Shot Chain-of-Thought Reasoning}} by {{Large Language Models}}},
  shorttitle = {Plan-and-{{Solve Prompting}}},
  author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
  year = {2023},
  month = may,
  number = {arXiv:2305.04091},
  eprint = {2305.04091},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.04091},
  urldate = {2025-04-19},
  abstract = {Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\CY4XM8WG\\Wang et al. - 2023 - Plan-and-Solve Prompting Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\LWYTD9W3\\2305.html}
}

@misc{wangSelfConsistencyImprovesChain2023,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2023},
  month = mar,
  number = {arXiv:2203.11171},
  eprint = {2203.11171},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.11171},
  urldate = {2025-05-01},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\Z977S5LC\\Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoning in Language Models.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\UYWQKVTB\\2203.html}
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  urldate = {2025-05-01},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\CM363QRD\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\9U45NG5F\\2201.html}
}

@inproceedings{wrightGeneratingScientificClaims2022,
  title = {Generating {{Scientific Claims}} for {{Zero-Shot Scientific Fact Checking}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wright, Dustin and Wadden, David and Lo, Kyle and Kuehl, Bailey and Cohan, Arman and Augenstein, Isabelle and Wang, Lucy Lu},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = {2022},
  month = may,
  pages = {2448--2460},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.175},
  urldate = {2025-04-12},
  abstract = {Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise. To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its usefulness in zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new supervised method for generating claims supported by the literature, as well as KBIN, a novel method for generating claim negations. Additionally, we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN, achieve up to 90\% performance of fully supervised models trained on manually annotated claims and evidence. A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines},
  file = {C:\Users\zahdehv\Zotero\storage\LIZV6FIT\Wright et al. - 2022 - Generating Scientific Claims for Zero-Shot Scientific Fact Checking.pdf}
}

@article{wuMindsEyeLLMs,
  title = {Mind's {{Eye}} of {{LLMs}}: {{Visualization-of-Thought Elicits Spatial Reasoning}} in {{Large Language Models}}},
  author = {Wu, Wenshan and Mao, Shaoguang and Zhang, Yadong and Xia, Yan and Dong, Li and Cui, Lei and Wei, Furu},
  abstract = {Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualizationof-Thought (VoT) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs. Please find the dataset and codes in our project page.},
  langid = {english},
  file = {C:\Users\zahdehv\Zotero\storage\B5BBM6YX\Wu et al. - Mind’s Eye of LLMs Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models.pdf}
}

@misc{yangBufferThoughtsThoughtAugmented2024,
  title = {Buffer of {{Thoughts}}: {{Thought-Augmented Reasoning}} with {{Large Language Models}}},
  shorttitle = {Buffer of {{Thoughts}}},
  author = {Yang, Ling and Yu, Zhaochen and Zhang, Tianjun and Cao, Shiyi and Xu, Minkai and Zhang, Wentao and Gonzalez, Joseph E. and Cui, Bin},
  year = {2024},
  month = oct,
  number = {arXiv:2406.04271},
  eprint = {2406.04271},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.04271},
  urldate = {2025-02-28},
  abstract = {We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11\% on Game of 24, 20\% on Geometric Shapes and 51\% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12\% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Notably, we find that our Llama3-8B+BoT has the potential to surpass Llama3-70B model. Our project is available at: https://github.com/YangLing0818/buffer-of-thought-llm},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\zahdehv\Zotero\storage\ZCWDYGCX\Yang et al. - 2024 - Buffer of Thoughts Thought-Augmented Reasoning with Large Language Models.pdf}
}

@misc{yangQwen251MTechnicalReport2025,
  title = {Qwen2.5-{{1M Technical Report}}},
  author = {Yang, An and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Huang, Haoyan and Jiang, Jiandong and Tu, Jianhong and Zhang, Jianwei and Zhou, Jingren and Lin, Junyang and Dang, Kai and Yang, Kexin and Yu, Le and Li, Mei and Sun, Minmin and Zhu, Qin and Men, Rui and He, Tao and Xu, Weijia and Yin, Wenbiao and Yu, Wenyuan and Qiu, Xiafei and Ren, Xingzhang and Yang, Xinlong and Li, Yong and Xu, Zhiying and Zhang, Zipeng},
  year = {2025},
  month = jan,
  number = {arXiv:2501.15383},
  eprint = {2501.15383},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.15383},
  urldate = {2025-02-28},
  abstract = {In this report, we introduce Qwen2.5-1M, a series of models that extend the context length to 1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series have significantly enhanced long-context capabilities through long-context pretraining and post-training. Key techniques such as long data synthesis, progressive pre-training, and multi-stage supervised fine-tuning are employed to effectively enhance long-context performance while reducing training costs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\QDCVAK9M\\Yang et al. - 2025 - Qwen2.5-1M Technical Report.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\BUAURL2A\\2501.html}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2025-04-25},
  abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\DUTJ73K9\Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models.pdf}
}

@misc{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  year = {2023},
  month = dec,
  number = {arXiv:2305.10601},
  eprint = {2305.10601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.10601},
  urldate = {2025-02-28},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, ``Tree of Thoughts'' (ToT), which generalizes over the popular ``Chain of Thought'' approach to prompting language models, and enables exploration over coherent units of text (``thoughts'') that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\F8ZL6LKB\Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with Large Language Models.pdf}
}

@misc{yasunagaLargeLanguageModels2024,
  title = {Large {{Language Models}} as {{Analogical Reasoners}}},
  author = {Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H. and Zhou, Denny},
  year = {2024},
  month = mar,
  number = {arXiv:2310.01714},
  eprint = {2310.01714},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01714},
  urldate = {2025-02-28},
  abstract = {Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual fewshot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\8MG6EJ2K\Yasunaga et al. - 2024 - Large Language Models as Analogical Reasoners.pdf}
}

@misc{zelikmanSTaRBootstrappingReasoning2022,
  title = {{{STaR}}: {{Bootstrapping Reasoning With Reasoning}}},
  shorttitle = {{{STaR}}},
  author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D.},
  year = {2022},
  month = may,
  number = {arXiv:2203.14465},
  eprint = {2203.14465},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.14465},
  urldate = {2025-02-28},
  abstract = {Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; finetune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to finetuning a 30{\texttimes} larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\zahdehv\Zotero\storage\E4RL5HDM\Zelikman et al. - 2022 - STaR Bootstrapping Reasoning With Reasoning.pdf}
}

@misc{zhengTakeStepBack2024,
  title = {Take a {{Step Back}}: {{Evoking Reasoning}} via {{Abstraction}} in {{Large Language Models}}},
  shorttitle = {Take a {{Step Back}}},
  author = {Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
  year = {2024},
  month = mar,
  number = {arXiv:2310.06117},
  eprint = {2310.06117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06117},
  urldate = {2025-05-02},
  abstract = {We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7\% and 11\% respectively, TimeQA by 27\%, and MuSiQue by 7\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\zahdehv\\Zotero\\storage\\PSU2WWXK\\Zheng et al. - 2024 - Take a Step Back Evoking Reasoning via Abstraction in Large Language Models.pdf;C\:\\Users\\zahdehv\\Zotero\\storage\\PCG79TPW\\2310.html}
}

@misc{zhouLeasttoMostPromptingEnables2023,
  title = {Least-to-{{Most Prompting Enables Complex Reasoning}} in {{Large Language Models}}},
  author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  year = {2023},
  month = apr,
  number = {arXiv:2205.10625},
  eprint = {2205.10625},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.10625},
  urldate = {2025-02-28},
  abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\zahdehv\Zotero\storage\SC3WLN4Z\Zhou et al. - 2023 - Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf}
}

@misc{zhuangSelfTaughtAgenticLong2025,
  title = {Self-{{Taught Agentic Long Context Understanding}}},
  author = {Zhuang, Yufan and Yu, Xiaodong and Wu, Jialian and Sun, Ximeng and Wang, Ze and Liu, Jiang and Su, Yusheng and Shang, Jingbo and Liu, Zicheng and Barsoum, Emad},
  year = {2025},
  month = feb,
  number = {arXiv:2502.15920},
  eprint = {2502.15920},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.15920},
  urldate = {2025-02-27},
  abstract = {Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chainof-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8\% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms stateof-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\zahdehv\Zotero\storage\NZPQTHCV\Zhuang et al. - 2025 - Self-Taught Agentic Long Context Understanding.pdf}
}
